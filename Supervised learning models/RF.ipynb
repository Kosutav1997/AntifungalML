{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1080 candidates, totalling 5400 fits\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages quietly\n",
    "!pip install rdkit scikit-learn pandas numpy -q\n",
    "\n",
    "# Import required libraries\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             roc_auc_score, matthews_corrcoef, balanced_accuracy_score)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the classification data\n",
    "df = pd.read_csv('top_30_features_from_mo.tsv', sep='\\t')\n",
    "\n",
    "# Handle missing values with imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[df.select_dtypes(include=[float, int]).columns] = imputer.fit_transform(df.select_dtypes(include=[float, int]))\n",
    "\n",
    "# Select numerical features and target variable\n",
    "numerical_features = df.select_dtypes(include=[float, int]).columns.drop(['Label'])  # Adjust column names as needed\n",
    "X = df[numerical_features].values\n",
    "y = df['Label'].values\n",
    "\n",
    "# Split data into training and testing sets with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0, stratify=y)\n",
    "\n",
    "# Scale numerical features (only if necessary)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30, 40],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Initialize RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# Perform Grid Search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters from Grid Search\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute evaluation metrics\n",
    "metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "    \"Balanced Accuracy\": balanced_accuracy_score(y_test, y_pred),\n",
    "    \"Precision\": precision_score(y_test, y_pred),\n",
    "    \"Recall\": recall_score(y_test, y_pred),\n",
    "    \"F1 Score\": f1_score(y_test, y_pred),\n",
    "    \"ROC AUC\": roc_auc_score(y_test, y_pred_proba),\n",
    "    \"MCC\": matthews_corrcoef(y_test, y_pred),\n",
    "}\n",
    "\n",
    "print(\"\\nTest Set Evaluation Metrics (Best Model):\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,\n",
    "                             roc_auc_score, roc_curve, matthews_corrcoef, balanced_accuracy_score, precision_score, recall_score, f1_score)\n",
    "from sklearn.impute import SimpleImputer\n",
    "from PIL import Image\n",
    "\n",
    "# Read data\n",
    "df = pd.read_csv('top_30_features_from_mo.tsv', sep='\\t')\n",
    "\n",
    "# Handle missing values by imputing with the mean\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X = df.drop(['Label', 'SMILES'], axis=1)\n",
    "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Extract labels\n",
    "y = df['Label'].values\n",
    "\n",
    "# Initialize scaler and classifier parameters\n",
    "scaler = StandardScaler()\n",
    "classifier_params = dict(n_estimators=300, random_state=0, bootstrap=False,\n",
    "                         max_depth=None, max_features='sqrt',\n",
    "                         min_samples_leaf=1, min_samples_split=2)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to collect metrics per fold\n",
    "acc_scores, auc_scores, mcc_scores, bal_acc_scores = [], [], [], []\n",
    "precision_scores, recall_scores, f1_scores = [], [], []\n",
    "\n",
    "# Store ROC curve points for each fold\n",
    "fold_rocs = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"\\n=== Fold {fold} ===\")\n",
    "    \n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Scale features\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train classifier\n",
    "    clf = RandomForestClassifier(**classifier_params)\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    y_pred_proba = clf.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='binary')\n",
    "    recall = recall_score(y_test, y_pred, average='binary')\n",
    "    f1 = f1_score(y_test, y_pred, average='binary')\n",
    "    \n",
    "    acc_scores.append(acc)\n",
    "    auc_scores.append(auc)\n",
    "    mcc_scores.append(mcc)\n",
    "    bal_acc_scores.append(bal_acc)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    print(\"Test Set Metrics:\")\n",
    "    print(f\"  Accuracy (ACC): {acc:.4f}\")\n",
    "    print(f\"  ROC AUC: {auc:.4f}\")\n",
    "    print(f\"  MCC: {mcc:.4f}\")\n",
    "    print(f\"  Balanced Accuracy: {bal_acc:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Save confusion matrix as image\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(6, 4.5))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    classes = np.unique(y)\n",
    "    ax.set(xticks=np.arange(len(classes)),\n",
    "           yticks=np.arange(len(classes)),\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=f'Fold {fold} Confusion Matrix',\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > cm.max() / 2. else \"black\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Save the image\n",
    "    png_path = f'confusion_matrix_fold_{fold}.png'\n",
    "    fig.savefig(png_path, dpi=600, format='png', pil_kwargs={\"optimize\": True, \"quality\": 95})\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Save ROC curve data for combined plot\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    fold_rocs.append((fpr, tpr, auc, fold))\n",
    "\n",
    "# Train final model on full data\n",
    "print(\"\\n=== Mean Model (Full Dataset) ===\")\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "clf_full = RandomForestClassifier(**classifier_params)\n",
    "clf_full.fit(X_scaled, y)\n",
    "\n",
    "y_pred_full = clf_full.predict(X_scaled)\n",
    "y_pred_proba_full = clf_full.predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "acc_full = accuracy_score(y, y_pred_full)\n",
    "auc_full = roc_auc_score(y, y_pred_proba_full)\n",
    "mcc_full = matthews_corrcoef(y, y_pred_full)\n",
    "bal_acc_full = balanced_accuracy_score(y, y_pred_full)\n",
    "precision_full = precision_score(y, y_pred_full, average='binary')\n",
    "recall_full = recall_score(y, y_pred_full, average='binary')\n",
    "f1_full = f1_score(y, y_pred_full, average='binary')\n",
    "\n",
    "# Save confusion matrix for mean model\n",
    "cm = confusion_matrix(y, y_pred_full)\n",
    "fig, ax = plt.subplots(figsize=(6, 4.5))\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "classes = np.unique(y)\n",
    "ax.set(xticks=np.arange(len(classes)),\n",
    "       yticks=np.arange(len(classes)),\n",
    "       xticklabels=classes, yticklabels=classes,\n",
    "       title='Mean Model Confusion Matrix',\n",
    "       ylabel='True label',\n",
    "       xlabel='Predicted label')\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > cm.max() / 2. else \"black\")\n",
    "fig.tight_layout()\n",
    "\n",
    "png_path = 'mean_model_confusion_matrix.png'\n",
    "fig.savefig(png_path, dpi=600, format='png', pil_kwargs={\"optimize\": True, \"quality\": 95})\n",
    "plt.close(fig)\n",
    "\n",
    "# Plot combined ROC curves and save\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "for fpr, tpr, auc, fold in fold_rocs:\n",
    "    ax.plot(fpr, tpr, linestyle=':', lw=2, label=f'Fold {fold} ROC (AUC = {auc:.4f})')\n",
    "\n",
    "fpr_full, tpr_full, _ = roc_curve(y, y_pred_proba_full)\n",
    "mean_auc = np.mean(auc_scores)  # Use mean AUC from CV folds here\n",
    "\n",
    "ax.plot(fpr_full, tpr_full, color='blue', lw=2,\n",
    "        label=f'Mean Model ROC (AUC = {mean_auc:.4f})', linestyle='-')\n",
    "ax.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', label='Chance')\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curves: 5-fold CV (dotted) & Mean Model (solid): RF')\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(alpha=0.3)\n",
    "fig.tight_layout()\n",
    "png_path = 'roc_curves.png'\n",
    "fig.savefig(png_path, dpi=600, format='png', pil_kwargs={\"optimize\": True, \"quality\": 95})\n",
    "plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate means and standard deviations for metrics\n",
    "metrics = {\n",
    "    \"Accuracy (ACC)\": (np.mean(acc_scores), np.std(acc_scores)),\n",
    "    \"ROC AUC\": (np.mean(auc_scores), np.std(auc_scores)),\n",
    "    \"MCC\": (np.mean(mcc_scores), np.std(mcc_scores)),\n",
    "    \"Balanced Accuracy\": (np.mean(bal_acc_scores), np.std(bal_acc_scores)),\n",
    "    \"Precision\": (np.mean(precision_scores), np.std(precision_scores)),\n",
    "    \"Recall\": (np.mean(recall_scores), np.std(recall_scores)),\n",
    "    \"F1 Score\": (np.mean(f1_scores), np.std(f1_scores)),\n",
    "}\n",
    "\n",
    "# Print metrics with mean ± std for cross-validation\n",
    "print(\"\\n=== Cross-validation Metrics (Mean ± Std) ===\")\n",
    "for metric, (mean, std) in metrics.items():\n",
    "    print(f\"{metric}: {mean:.4f} ± {std:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
